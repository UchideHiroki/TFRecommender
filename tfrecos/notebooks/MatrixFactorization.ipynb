{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ざっくりKerasでMFを書いちゃう  \n",
    "→datasetの複数入力受け渡しが面倒臭い、やはり信頼できるのは生TF  \n",
    "信頼、というか、責任が全て自分にある"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from collections import defaultdict\n",
    "from copy import copy, deepcopy\n",
    "import os\n",
    "from datetime import datetime\n",
    "import random\n",
    "import math\n",
    "\n",
    "np.random.seed(1234)\n",
    "sns.set_style(\"darkgrid\")\n",
    "pd.set_option(\"display.max_columns\", 100)\n",
    "pd.set_option(\"display.max_rows\", 500)\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_formats = {\"png\", \"retina\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from logging import getLogger\n",
    "logger = getLogger(__name__)\n",
    "import sklearn\n",
    "from typing import List, Any, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(tf.__version__)\n",
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = tf.ConfigProto()\n",
    "# config.gpu_options.allow_growth = True\n",
    "# config.gpu_options.per_process_gpu_memory_fraction = 0.4\n",
    "# sess = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/device:GPU:0'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "モジュールのpyファイルの変更がリアルタイムに反映されるようにする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasetの読み込み"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MovieLenzを使用  \n",
    "Rendle_et_al_2009(BPR)に従って、レーティングデータを抜く  \n",
    "映画を見るか見ないかの2値分類を、映画を見たというログのみから当てにいく  \n",
    "BPRではMovieLenzを使用    \n",
    "10以上のitemをratingしたユーザー  \n",
    "10以上のuserにratingされたitem  \n",
    "に絞って、10000users, 5000items, 565738ratingsが集まった  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterator・Datasetの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_data = pd.read_csv(\"../input/movielenz/ratings.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(671, 9066)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_data[\"userId\"].nunique(), log_data[\"movieId\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">movieId</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>nunique</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>149.037258</td>\n",
       "      <td>149.037258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>median</th>\n",
       "      <td>71.000000</td>\n",
       "      <td>71.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2391.000000</td>\n",
       "      <td>2391.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            movieId             \n",
       "              count      nunique\n",
       "min       20.000000    20.000000\n",
       "mean     149.037258   149.037258\n",
       "median    71.000000    71.000000\n",
       "max     2391.000000  2391.000000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_data.groupby([\"userId\"])[[\"movieId\"]].aggregate([\"count\", \"nunique\"]).aggregate([\"min\", \"mean\", \"median\", \"max\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一人あたり最低20, 中央値71, 最大2391ratingしている"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>11.030664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>median</th>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>341.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            userId\n",
       "min       1.000000\n",
       "mean     11.030664\n",
       "median    3.000000\n",
       "max     341.000000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_data.groupby([\"movieId\"])[[\"userId\"]].count().aggregate([\"min\", \"mean\", \"median\", \"max\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1映画あたり最低1、中央値3, 最大341人からratingを受けている"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(object):\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.n_data = len(data)\n",
    "        self.n_users = len(set(data[\"userId\"]))\n",
    "        self.n_items = len(set(data[\"movieId\"]))\n",
    "        self.user_ids = list(data[\"userId\"])\n",
    "        self.item_ids = list(data[\"movieId\"])\n",
    "        self.user2index = dict(zip(np.unique(self.user_ids), range(self.n_users)))\n",
    "        self.item2index = dict(zip(np.unique(self.item_ids), range(self.n_items)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset(log_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100004, 671, 9066)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.n_data, dataset.n_users, dataset.n_items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/root/docker/tfrecos/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tfrecos as tfr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tfr.model.MatrixFactorization(\n",
    "    n_latent_factors=10,\n",
    "    learning_rate=0.01,\n",
    "    reg_user=0.001,\n",
    "    reg_item=0.001,\n",
    "    batch_size=1000,\n",
    "    epoch_size=50,\n",
    "    test_size=0.1,\n",
    "    save_directory_path=\"../logs/20190712_MF\",\n",
    "    scope_name=\"MF\",\n",
    "    try_count=5,\n",
    "    n_users=dataset.n_users,\n",
    "    n_items=dataset.n_items,\n",
    "    user2index=dataset.user2index,\n",
    "    item2index=dataset.item2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = \"../logs/20190712_MF/checkpoint/model_3.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please call load_weights instead\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "None",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-b20ff37ba1a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mckpt_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/docker/tfrecos/tfrecos/model/matrix_factorization.py\u001b[0m in \u001b[0;36mbuild_model\u001b[0;34m(self, checkpoint_path)\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_users\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Please set n_users when init\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_items\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Please set n_items when init\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Please call load_weights instead\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: None"
     ]
    }
   ],
   "source": [
    "model.build_model(ckpt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "latest = tf.train.latest_checkpoint(model.checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../logs/20190712_MF/checkpoint/model_19.ckpt\n"
     ]
    }
   ],
   "source": [
    "model.load_weights(latest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset out of range\n",
      "0.6104006 0.60147685\n",
      "valid_dataset out of range\n",
      "0.5779597 0.5658795\n",
      "INFO:tensorflow:../logs/20190712_MF/checkpoint/model_0.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "train_dataset out of range\n",
      "0.5574784 0.5436275\n",
      "valid_dataset out of range\n",
      "0.5403211 0.52565765\n",
      "INFO:tensorflow:../logs/20190712_MF/checkpoint/model_1.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "train_dataset out of range\n",
      "0.48243958 0.46333647\n",
      "valid_dataset out of range\n",
      "0.4436193 0.42003465\n",
      "INFO:tensorflow:../logs/20190712_MF/checkpoint/model_2.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "train_dataset out of range\n",
      "0.3879348 0.35938638\n",
      "valid_dataset out of range\n",
      "0.38026455 0.3503985\n",
      "INFO:tensorflow:../logs/20190712_MF/checkpoint/model_3.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "train_dataset out of range\n",
      "0.3691074 0.33654755\n",
      "valid_dataset out of range\n",
      "0.39172578 0.35916933\n",
      "INFO:tensorflow:../logs/20190712_MF/checkpoint/model_4.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "train_dataset out of range\n",
      "0.35565886 0.320406\n",
      "valid_dataset out of range\n",
      "0.36548743 0.33017942\n",
      "INFO:tensorflow:../logs/20190712_MF/checkpoint/model_5.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "train_dataset out of range\n",
      "0.34040892 0.30206582\n",
      "valid_dataset out of range\n",
      "0.3775452 0.33876386\n",
      "INFO:tensorflow:../logs/20190712_MF/checkpoint/model_6.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "train_dataset out of range\n",
      "0.3261246 0.28408694\n",
      "valid_dataset out of range\n",
      "0.35845295 0.3165049\n",
      "INFO:tensorflow:../logs/20190712_MF/checkpoint/model_7.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "train_dataset out of range\n",
      "0.31605598 0.27057466\n",
      "valid_dataset out of range\n",
      "0.3659654 0.32072696\n",
      "INFO:tensorflow:../logs/20190712_MF/checkpoint/model_8.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "train_dataset out of range\n",
      "0.30938953 0.26076007\n",
      "valid_dataset out of range\n",
      "0.37625086 0.32812732\n",
      "INFO:tensorflow:../logs/20190712_MF/checkpoint/model_9.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "train_dataset out of range\n",
      "0.30133286 0.24976884\n",
      "valid_dataset out of range\n",
      "0.37317833 0.32236585\n",
      "INFO:tensorflow:../logs/20190712_MF/checkpoint/model_10.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "train_dataset out of range\n",
      "0.29271132 0.23840572\n",
      "valid_dataset out of range\n",
      "0.3570424 0.30362707\n",
      "INFO:tensorflow:../logs/20190712_MF/checkpoint/model_11.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "train_dataset out of range\n",
      "0.2903231 0.23358835\n",
      "valid_dataset out of range\n",
      "0.4039977 0.34862757\n",
      "INFO:tensorflow:../logs/20190712_MF/checkpoint/model_12.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "train_dataset out of range\n",
      "0.2850722 0.22623646\n",
      "valid_dataset out of range\n",
      "0.35242042 0.2952045\n",
      "INFO:tensorflow:../logs/20190712_MF/checkpoint/model_13.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "train_dataset out of range\n",
      "0.2820733 0.22146463\n",
      "valid_dataset out of range\n",
      "0.36459908 0.3057587\n",
      "INFO:tensorflow:../logs/20190712_MF/checkpoint/model_14.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "train_dataset out of range\n",
      "0.28083605 0.21871093\n",
      "valid_dataset out of range\n",
      "0.42153504 0.36122438\n",
      "INFO:tensorflow:../logs/20190712_MF/checkpoint/model_15.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "train_dataset out of range\n",
      "0.2785246 0.21497665\n",
      "valid_dataset out of range\n",
      "0.38490644 0.3232904\n",
      "INFO:tensorflow:../logs/20190712_MF/checkpoint/model_16.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "train_dataset out of range\n",
      "0.27488756 0.21002421\n",
      "valid_dataset out of range\n",
      "0.3748034 0.31206253\n",
      "INFO:tensorflow:../logs/20190712_MF/checkpoint/model_17.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "train_dataset out of range\n",
      "0.274706 0.2086467\n",
      "valid_dataset out of range\n",
      "0.36713827 0.3035314\n",
      "INFO:tensorflow:../logs/20190712_MF/checkpoint/model_18.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "train_dataset out of range\n",
      "0.27424157 0.20735973\n",
      "valid_dataset out of range\n",
      "0.38004205 0.31568986\n",
      "INFO:tensorflow:../logs/20190712_MF/checkpoint/model_19.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "model.fit(user_ids=dataset.user_ids, item_ids=dataset.item_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_factors = model.get_item_factors(item_ids=dataset.item_ids,\n",
    "                                     normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100004, 10)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_factors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_factors = model.get_user_factors(user_ids=dataset.user_ids[:10],\n",
    "                                     normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_factors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(user_ids=dataset.user_ids,\n",
    "                            item_ids=dataset.item_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8686248044368201"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "clickがされているデータに対しての予測値の平均が0.8→それなりに当たっている"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
